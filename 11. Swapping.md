# Swapping
---
> 우리는 모든 것을 physical memory에 저장하고 싶지만, physical memory의 크기에는 한계가 있기 때문에     
> 지금 당장 사용하지 않는 address space를 어딘가에 저장해주어야 한다.      
> 그리고 이것을 __Memory Hierarchy__ 구조라고 한다.

![image](https://user-images.githubusercontent.com/80378041/165474478-0929bd87-3f11-4d26-b2b7-452f620dfda2.png)

## How to Swap
> 위쪽 계층에서 아래 계층으로 page를 swap하는 데는 3가지 방법이 있다.

- Overlays
  - 프로그래머가 직접 코드레벨에서 메모리를 관리하는 방법
- Process-level swapping
  - 프로세스 전체를 backing store에 옮기는 것
  - 보통 I/O Event를 기다릴 때 사용한다
- Page-level swapping
  - 지금 당장 필요없는 페이지를 내린다
  - swap-out : page를 backing store로 옮기는 것
  - swap-in : backing store에서 page를 가지고 오는 것

## Where to Swap
- Swap space
  - swap space의 크기가 사용할 수 있는 메모리 페이지의 최대 개수를 정해준다
  - 블럭 사이즈는 page 사이즈와 동일하다

![image](https://user-images.githubusercontent.com/80378041/165475482-18002cee-c1d0-4523-bd1c-ad51394af768.png)

## Single large address for a process
> 이러한 Swapping 방식은 각각의 process가 대용량의 메모리를 가지고 있다고 생각하게 해준다

## Present Bit
> Page가 physical memory 안에 있는지, 아니면 disk에 있는지를 구분하기 위해 사용하는 bit.      
> 1이면 physical memory 안에 존재하고, 0이면 disk에 존재한다

## Page Fault
> present bit가 0이면, 해당 페이지가 physical memory 안에 없는 것이기 때문에     
> OS가 페이지를 디스크에서 가져와야 한다.
 
 ![image](https://user-images.githubusercontent.com/80378041/165476152-93d1538a-7ae5-428e-bc2f-dc2e3c50514b.png)

## If Memory is Full
> 만약 메모리가 가득 찼으면, 임의의 Page를 메모리에서 쫒아내고, 새로운 Page를 가져와야 한다.      
> 하지만, 하나씩 계속 교체해주는 방법을 사용하게 되면, overhead가 많이 발생하게 되고,      
> 따라서 현실에서 하나씩 교체하는 방법을 사용하긴 힘들다.

### Swap Daemon, Page Daemon
> 그래서 나온 방법이 항상 일정 이상의 free 메모리 공간이 있도록 유지해주는 방법이다.     
> Low watermark 와 High watermark를 지정해주고, free page가 LW보다 작으면 memory에서 Page를 쫒아내고 공간을 확보한다.      
> 이 때, HW에 도달할 때까지 공간을 확보하게 된다.

### What to Swap
> 그렇다면, 어떤 페이지들을 쫒아낼 수 있을까?

![image](https://user-images.githubusercontent.com/80378041/165477121-b3cdc5bf-744b-4103-9a38-e0d1d9abe96c.png)

- Not swapped는 항상 메모리에 존재해야하기 때문에 Swap이 되지 않는다.
- Dropped는 Disk나 file system에 저장되어있는 경우에 Swap을 하지 않고 버려준다
- User에 의해 변경되어서 저장할 필요가 있을 때는 Swap을 해준다

## Policies
> 그렇다면, 누구를 쫒아낼 것인지에 대한 법칙도 필요해진다.      
> 보통 Cache Managment `P_hit * T_m + P_miss * T_d`가 가장 높게 나오는 방법을 선택하고 싶을 것이다.

### Optimal Replacement Policy
> 가장 먼 미래에 access되는 page를 쫒아내는 방법이다.      
> 이론적으로는 가장 miss가 적은 방법이지만, 미래를 알아야 하기 때문에 현실에서는 구현해낼 수 없다.     
> 다른 방법에 대한 비교 대상으로만 사용된다.

![image](https://user-images.githubusercontent.com/80378041/165478139-df9640a5-8337-41e9-9b0f-1587a2ae7934.png)

### FIFO(First In First Out)
> 먼저 메모리에 올라온 것을 쫒아내는 방법이다.     
> 중요한 block을 구분해서 쫒아낼 수 없는 단점이 있다.

![image](https://user-images.githubusercontent.com/80378041/165478395-9de2a26e-ba63-4492-bf8b-3b01db40e896.png)


### BELADY's ANOMALY
> 캐시의 크기를 키우면 Hit가 많아질 것이라고 생각하지만, miss가 많아지는 현상이다.     
> FIFO가 해당된다

![image](https://user-images.githubusercontent.com/80378041/165478700-9a033652-2e62-4030-a6ed-526f0c7930b7.png)

### Random
> 말 그대로 무작위로 쫒아내는 방법이다.     
> 운만 좋다면, Optimal과 동일한 성능을 낼 수도 있다.

## Using History
> 이제는, 과거를 보고 미래를 예측하는 방법들이다.     
> recency와 frequency정보를 보고 미래를 예측하는 2가지 방법이 있다

### LRU(Least Recently Used)
> 사용한지 가장 오래된 것을 쫒아내는 방법이다.

- temporal locality와 잘 맞는다
- BELADY's ANOMALY에 영향을 받지 않는다
- frequency는 고려하지 않는다
- 모든 workload에 적합한 것은 아니다
- 구현하기 어렵고, 완벽 구현은 사실상 불가능하다

![image](https://user-images.githubusercontent.com/80378041/165479399-16b37f1d-f62d-4504-a85c-806d83ca80b5.png)

> 그렇다면, 어떤 경우에 LRU가 효율적일까?

## Workload Example

### No-Locality Workload
> 만약 100개의 일이 있다고 가정하자. 이 때, 모든 일이 페이지를 랜덤하게 접근한다면?     
> ![image](https://user-images.githubusercontent.com/80378041/166253065-4fcae037-1579-4b6e-a161-aa4f0742cd2b.png)     
> 위의 사진처럼 랜덤하게 접근 할 경우에는 LRU든 FIFO든 차이가 없다.     

### 80-20 Workload
> 다음으로 80%의 일이 20%의 페이지를 참조해 일을 하고, 20%의 일이 나머지 80%의 페이지를 참조한다면?      
> ![image](https://user-images.githubusercontent.com/80378041/166253417-faa9d253-504f-478d-9c44-ddbf3ea214db.png)     
> 위의 사진처럼 FIFO나 RAND와는 다르게 LRU가 제 구실을 할 수 있게 된다.      
> 이 때 많이 참조되는 20%의 페이지를 hot page라고 한다

### The Looping Sequential
> 1~50까지의 페이지를 순서대로, 반복해서 접근하는 Loop의 형태라면?      
> ![image](https://user-images.githubusercontent.com/80378041/166253862-748a11b5-ab2f-4a8d-8e33-4198c687e045.png)     
> 위의 사진처럼 RAND를 제외하고는 cache size가 50이 될 때까지 LRU와 FIFO는 miss만 나게 될 것이다.      
> 하지만, 저정도로 큰 Loop를 현실에서 만날 일은 없기 때문에 걱정하지 않아도 된다.

## Historical Algorithm의 구현
> 계속 접근했던 시간을 저장하게 된다면 overhead가 상당히 커지게 될 것이다.     
> 이것을 해소하기 위해 하드웨의 도움이 필요하게 된다.

### Approximation LRU
> 원래는 접근 시간을 저장해야 하지만, 그렇게 되면 overhead가 너무 커지기 때문에 use bit를 만들어     
> 접근을 했었는지만 저장해준다.      
> 이 때, use bit는 하드웨어에서 초기화 하지 않고, OS가 나중에 초기화를 하게 된다

### Clock Algorithm
> 모든 페이지들이 원형으로 관리되고, clock hand가 다음에 시작할 특정 페이지를 가리키는 방법이다.     
> ![image](https://user-images.githubusercontent.com/80378041/166254630-5d37f84f-105c-451f-b982-6c4af560795e.png)     
> 이 때, 메모리가 모자라게 된다면 use bit가 0인 것을 찾아서 쫒아내게 된다.      

### Clock Algorithm
> ![image](https://user-images.githubusercontent.com/80378041/166255110-f8e0d3d9-486f-47b7-9d64-84bc65eafaa9.png)     
> LRU만큼 완벽하진 않지만, 그래도 다른 것들보다는 나은 성능을 가지고 있다.

## Dirty Page
> 메모리에서 쫒아낼 때, 그 페이지가 수정되었으면 어떻게 해야 할까?
> 페이지가 수정되었으면 dirty bit를 1로 만들어주고, 메모리에서 쫒아내기 전에      
> 디스크에 옮겨 적어야 한다.     
> dirty bit가 0이면 디스크에 이미 있으므로 그냥 free 해주어도 된다.

## Page selection Policy
> 페이지를 불러올 때, 지금 필요한 페이지만 불러오게 된다면 사용자가 많이 답답함을 느낄 것이다.     
> 그렇기 때문에 페이지를 불러올 때 몇 가지 방법을 만들었다.

- Prefetching
  - 어떤 페이지가 사용될 지 파악하고, 미리 읽어오는 것이다.
  - 예를 들어, 1번 페이지 다음에는 2번 페이지를 사용할 것을 예측해서 1번과 2번 페이지를 모두 메모리로 가져오는 것이다
- Clustering, Grouping
  - 여러개의 페이지를 묶어서 한번에 메모리로 가져오는 것이다
  - 한번에 여러개를 가져오기 때문에 상당히 효율적이다

## Thrashing
> 한번에 너무 많은 프로세스를 사용중이라 메모리가 부족한 경우에 발생한다.      
> 이 때, CPU는 계속 메모리에서 Page refreshing을 하게 된다.      

![image](https://user-images.githubusercontent.com/80378041/166256022-1527563e-952c-42b0-962d-9bfcb0e5031c.png)
